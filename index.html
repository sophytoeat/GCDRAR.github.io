<!DOCTYPE html>
<html>
<head>
  <title>Collaborative Garment Design using Remote Augmented Reality</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Collaborative Garment Design using Remote Augmented Reality</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Dávid Maruscsák</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a>Yechen LI</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/index.html">Takeo Igarashi</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://drsandor.net/">Christian Sandor</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>CNRS</span>
            <span class="author-block"><sup>2</sup>Université Paris-Saclay</span>
            <span class="author-block"><sup>3</sup>The University of Tokyo</span>
          </div>

          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/teaser.png">
        
      </img>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary Statement</h2>
        <div class="content has-text-justified">
          <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Technical Overview</h2>
        <div class="content has-text-justified">
          <p>
            We capture the front part of the "Client's" body using two Kinect Azure cameras. For the client, we use an AR mirror to superimpose the virtual garment on its body. We use a TV screen (230cm x 130cm) in portrait mode. To send the client's physical data to the designer, first, we extract the RGB and Depth Camera data plus the skeleton Technical overview - Client side:
data from the Kinects. We send the skeleton data as OSC messages (6-DOF float messages/joint). We use NDI to send the RGB/Depth camera data.<br>
            Designer side:
The Designer uses a desktop computer with a drawing tablet and a pen. We use Unreal Engine (UE) to visualise the client's body. The skeleton data drives a skeletal mesh in UE. We use the RGB video as a background image and the depth data to provide correct occlusions and collisions between the garment and the client's body. The cloth simulation system in UE communicates with Marvelous Designer (MD), a software for cloth tailoring. In MD, the Designer can create new clothes (e.g. T-shirts, trousers) and edit them, including trimming and cutting. These changes automatically reflect in UE.<br>
            Networking:
To make a stable connection between the two remote desktop PCs we use a private VPN connection. On the client's side, we process the Kinect Azures' data in TouchDesigner. To effectively send the Kinects' data (Skeleton Data as OSC float messages; RGB Camera feed as 8-bit NDI stream; Depth Camera feed as 16-bit NDI stream), we use NDI Bridge for converting the NDI streams and achieve fast transferring speed (numbers in milliseconds).
          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/g_xz86Lrdl0?si=qZo61gCQaPYn8o7c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

</body>
</html>
